{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0c60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr as eo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00819044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ExifTags\n",
    "import numpy as np\n",
    "\n",
    "def get_oriented_image_array(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image, checks its EXIF orientation metadata, and applies the correct rotation.\n",
    "    Returns a NumPy array suitable for OCR or OpenCV processing.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    try:\n",
    "        for orientation in ExifTags.TAGS:\n",
    "            if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "                break\n",
    "\n",
    "        exif = image._getexif()\n",
    "        if exif is not None:\n",
    "            orientation_value = exif.get(orientation, None)\n",
    "\n",
    "            if orientation_value == 3:\n",
    "                image = image.rotate(180, expand=True)\n",
    "            elif orientation_value == 6:\n",
    "                image = image.rotate(270, expand=True)\n",
    "            elif orientation_value == 8:\n",
    "                image = image.rotate(90, expand=True)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùó Orientation adjustment skipped:\", e)\n",
    "\n",
    "    return np.array(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9496b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_ocr(image_np):\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Resize if needed (EasyOCR prefers readable font size)\n",
    "    if gray.shape[0] < 1000:\n",
    "        gray = cv2.resize(gray, None, fx=2.0, fy=2.0, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Slight denoising\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding to make text pop\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 31, 15\n",
    "    )\n",
    "\n",
    "    return thresh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54053731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: Cut into Slices ===\n",
    "def slice_image(image_np, slice_height_ratio=0.3, overlap_ratio=0.3):\n",
    "    h, w = image_np.shape[:2]\n",
    "    slice_h = int(h * slice_height_ratio)\n",
    "    overlap = int(slice_h * overlap_ratio)\n",
    "    \n",
    "    slices = []\n",
    "    positions = []\n",
    "    y = 0\n",
    "    while y < h:\n",
    "        y_end = min(y + slice_h, h)\n",
    "        slices.append(image_np[y:y_end, :])\n",
    "        positions.append((y, y_end))\n",
    "        y += slice_h - overlap  # move down with overlap\n",
    "    return slices, positions\n",
    "\n",
    "# === STEP 4: Run OCR on All Slices ===\n",
    "def run_ocr_on_slices(reader, slices, positions):\n",
    "    all_results = []\n",
    "    for idx, (img_slice, (y1, y2)) in enumerate(zip(slices, positions)):\n",
    "        results = reader.readtext(img_slice)\n",
    "        for bbox, text, conf in results:\n",
    "            # Shift bbox vertically back to full image coordinates\n",
    "            shifted_bbox = [(x, y + y1) for (x, y) in bbox]\n",
    "            all_results.append((shifted_bbox, text, conf))\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a880211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: Display Results ===\n",
    "def display_results(results, image_np, result_path):\n",
    "    image = image_np.copy()\n",
    "    for bbox, text, confidence in results:\n",
    "        pts = [tuple(map(int, point)) for point in bbox]\n",
    "        cv2.polylines(image, [np.array(pts)], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "        cv2.putText(image, text, pts[0], cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"OCR Results\")\n",
    "    plt.show()\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    cv2.imwrite(f'results/{result_path}.jpg', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a3ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m image_np = get_oriented_image_array(img_path)\n\u001b[32m      4\u001b[39m preprocessed_img = preprocess_for_ocr(image_np)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m reader = \u001b[43meo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m slices, positions = slice_image(preprocessed_img)\n\u001b[32m      9\u001b[39m results = run_ocr_on_slices(reader, slices, positions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\easyocr\\easyocr.py:214\u001b[39m, in \u001b[36mReader.__init__\u001b[39m\u001b[34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[39m\n\u001b[32m    211\u001b[39m     dict_list[lang] = os.path.join(BASE_PATH, \u001b[33m'\u001b[39m\u001b[33mdict\u001b[39m\u001b[33m'\u001b[39m, lang + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m detector:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28mself\u001b[39m.detector = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recognizer:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recog_network == \u001b[33m'\u001b[39m\u001b[33mgeneration1\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\easyocr\\easyocr.py:271\u001b[39m, in \u001b[36mReader.initDetector\u001b[39m\u001b[34m(self, detector_path)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitDetector\u001b[39m(\u001b[38;5;28mself\u001b[39m, detector_path):\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcudnn_benchmark\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcudnn_benchmark\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\easyocr\\detection.py:75\u001b[39m, in \u001b[36mget_detector\u001b[39m\u001b[34m(trained_model, device, quantize, cudnn_benchmark)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_detector\u001b[39m(trained_model, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, quantize=\u001b[38;5;28;01mTrue\u001b[39;00m, cudnn_benchmark=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     net = \u001b[43mCRAFT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     78\u001b[39m         net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\easyocr\\craft.py:35\u001b[39m, in \u001b[36mCRAFT.__init__\u001b[39m\u001b[34m(self, pretrained, freeze)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28msuper\u001b[39m(CRAFT, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Base network \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.basenet = \u001b[43mvgg16_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" U network \"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.upconv1 = double_conv(\u001b[32m1024\u001b[39m, \u001b[32m512\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\easyocr\\model\\modules.py:27\u001b[39m, in \u001b[36mvgg16_bn.__init__\u001b[39m\u001b[34m(self, pretrained, freeze)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28msuper\u001b[39m(vgg16_bn, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(torchvision.__version__) >= version.parse(\u001b[33m'\u001b[39m\u001b[33m0.13\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     vgg_pretrained_features = \u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvgg16_bn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVGG16_BN_Weights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEFAULT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.features\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#torchvision.__version__ < 0.13\u001b[39;00m\n\u001b[32m     31\u001b[39m     models.vgg.model_urls[\u001b[33m'\u001b[39m\u001b[33mvgg16_bn\u001b[39m\u001b[33m'\u001b[39m] = models.vgg.model_urls[\u001b[33m'\u001b[39m\u001b[33mvgg16_bn\u001b[39m\u001b[33m'\u001b[39m].replace(\u001b[33m'\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp://\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[39m, in \u001b[36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     warnings.warn(\n\u001b[32m    136\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs.keys()),\u001b[38;5;250m \u001b[39mseparate_last=\u001b[33m'\u001b[39m\u001b[33mand \u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as positional \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     kwargs.update(keyword_only_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[39m, in \u001b[36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[32m    226\u001b[39m     kwargs[weights_param] = default_weights_arg\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torchvision\\models\\vgg.py:459\u001b[39m, in \u001b[36mvgg16_bn\u001b[39m\u001b[34m(weights, progress, **kwargs)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"VGG-16-BN from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[32m    440\u001b[39m \n\u001b[32m    441\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    455\u001b[39m \u001b[33;03m    :members:\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m weights = VGG16_BN_Weights.verify(weights)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torchvision\\models\\vgg.py:103\u001b[39m, in \u001b[36m_vgg\u001b[39m\u001b[34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weights.meta[\u001b[33m\"\u001b[39m\u001b[33mcategories\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m         _ovewrite_named_param(kwargs, \u001b[33m\"\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights.meta[\u001b[33m\"\u001b[39m\u001b[33mcategories\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m model = \u001b[43mVGG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torchvision\\models\\vgg.py:62\u001b[39m, in \u001b[36mVGG.__init__\u001b[39m\u001b[34m(self, features, num_classes, init_weights, dropout)\u001b[39m\n\u001b[32m     60\u001b[39m     nn.init.constant_(m.bias, \u001b[32m0\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn.Linear):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     nn.init.constant_(m.bias, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torch\\nn\\init.py:193\u001b[39m, in \u001b[36mnormal_\u001b[39m\u001b[34m(tensor, mean, std, generator)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.overrides.has_torch_function_variadic(tensor):\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.overrides.handle_torch_function(\n\u001b[32m    191\u001b[39m         normal_, (tensor,), tensor=tensor, mean=mean, std=std, generator=generator\n\u001b[32m    192\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iliyan Tashinov\\Desktop\\open-shelf\\venv\\Lib\\site-packages\\torch\\nn\\init.py:22\u001b[39m, in \u001b[36m_no_grad_normal_\u001b[39m\u001b[34m(tensor, mean, std, generator)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_normal_\u001b[39m(tensor, mean, std, generator=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === MAIN EXECUTION ===\n",
    "img_path = 'fantastiko/fantastiko_1.jpeg'\n",
    "image_np = get_oriented_image_array(img_path)\n",
    "preprocessed_img = preprocess_for_ocr(image_np)\n",
    "\n",
    "reader = eo.Reader(['en', 'bg'])\n",
    "\n",
    "slices, positions = slice_image(preprocessed_img)\n",
    "results = run_ocr_on_slices(reader, slices, positions)\n",
    "\n",
    "display_results(results, image_np, 'fantastiko_2_sliced')\n",
    "\n",
    "# Print text results\n",
    "for bbox, text, conf in results:\n",
    "    pts = [tuple(map(int, point)) for point in bbox]\n",
    "    print(f'{pts} - {text} (conf: {conf:.2f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
